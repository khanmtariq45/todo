import os import re import sys import urllib.parse from datetime import datetime from docx import Document from win32com import client import pyodbc

Logging setup

LOG_FILE = f"link_update_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt" def log(msg): with open(LOG_FILE, "a", encoding="utf-8") as f: f.write(msg + "\n") print(msg)

Regex patterns

URL_PATTERN = ( r'(https?://[^\s<>"'{}|\^[]+|www\.[^\s<>"\'{}|\\^[]+|' r'file://[^\s<>"'{}|\^[]+|tel:[^\s<>"\'{}|\\^[]+)' ) LOCAL_FILE_PATTERN = ( r'(file://[^\s<>"'{}|\^\]]+|[A-Za-z]:[\\/][^\s<>"\'{}|\\^]]+|' r'(?:.?.?[\/]|[^:/\\s<>|]+[\/])[^\s<>"'{}|\^`]]+)' ) URL_REGEX = re.compile(URL_PATTERN, re.IGNORECASE) LOCAL_FILE_REGEX = re.compile(LOCAL_FILE_PATTERN, re.IGNORECASE) EXCLUDE_PREFIXES = ("http://", "https://", "mailto:", "tel:", "ftp://", "s://", "www.")

def fetch_qms_file_id(filepath): dbConnectionString = ( "DRIVER={ODBC Driver 17 for SQL Server};" "SERVER=dev.c5owyuw64shd.ap-south-1.rds.amazonaws.com,1982;" "DATABASE=JIBE_Main;" "UID=j2;" "PWD=123456;" "Max Pool Size=200;" ) try: conn = pyodbc.connect(dbConnectionString) cursor = conn.cursor() query = """ SELECT TOP 1 encryptedDocId FROM QMS_DocIds_Import01 WHERE filepath LIKE ? """ normalized_path = filepath.replace("\", "/").lower() cursor.execute(query, f"%{normalized_path}%") row = cursor.fetchone() if row: log(f"Found encryptedDocId for {filepath}: {row[0]}") else: log(f"No encryptedDocId found for {filepath}") return row[0] if row else None except Exception as e: log(f"Database error for {filepath}: {e}") return None finally: try: cursor.close() conn.close() except: pass

def is_local_file_url(url): url = url.strip().lower() return (LOCAL_FILE_REGEX.match(url) and not url.startswith(EXCLUDE_PREFIXES))

def get_qms_replacement(url): encrypted_doc_id = fetch_qms_file_id(url) if encrypted_doc_id: return f"#\qms?DocId={encrypted_doc_id}" return None

def process_hyperlink(hyperlink, line_offset, source_type): try: if not (hyperlink and hasattr(hyperlink, 'address') and hyperlink.address): return None url = hyperlink.address.strip() if not is_local_file_url(url): return None replacement = get_qms_replacement(url) if not replacement: log(f"Replacement not found for {url}") return None log(f"Will replace {url} with {replacement}") display_text = (hyperlink.text.strip() if hasattr(hyperlink, 'text') and hyperlink.text else replacement) return (url, replacement, line_offset, source_type, display_text) except Exception as e: log(f"Warning: Hyperlink error - {e}") return None

def extract_links_from_text(text, line_offset, existing_links): links = [] if not text.strip(): return links for url in URL_REGEX.findall(text): if url and not any(url in found_url for found_url, *_ in existing_links): if is_local_file_url(url): replacement = get_qms_replacement(url) if replacement: log(f"Text URL: Will replace {url} with {replacement}") links.append((url, replacement, line_offset, "Text", replacement)) return links

def extract_paragraph_links(paragraph, line_offset): links = [] text = paragraph.text.strip() if not text: return links if hasattr(paragraph, 'hyperlinks'): for hyperlink in paragraph.hyperlinks: link_data = process_hyperlink(hyperlink, line_offset, "Hyperlink") if link_data: links.append(link_data) links.extend(extract_links_from_text(text, line_offset, links)) return links

def extract_docx_links(file_path): links = [] try: doc = Document(file_path) line_offset = 0 for para in doc.paragraphs: links.extend(extract_paragraph_links(para, line_offset)) line_offset += len(para.text.split('\n')) for table in doc.tables: for row in table.rows: for cell in row.cells: for para in cell.paragraphs: links.extend(extract_paragraph_links(para, line_offset)) line_offset += len(para.text.split('\n')) for section in doc.sections: for part in (section.header, section.footer): if part: for para in part.paragraphs: links.extend(extract_paragraph_links(para, line_offset)) line_offset += len(para.text.split('\n')) except Exception as e: log(f"Error extracting links from {file_path}: {e}") return links

def update_docx_file(file_path, links_to_update): try: doc = Document(file_path) updated = False for para in doc.paragraphs: if not para.text.strip(): continue for original, replacement, *_ in links_to_update: if original in para.text: para.text = para.text.replace(original, replacement) updated = True log(f"Updated in paragraph: {original} => {replacement}") for table in doc.tables: for row in table.rows: for cell in row.cells: for para in cell.paragraphs: for original, replacement, *_ in links_to_update: if original in para.text: para.text = para.text.replace(original, replacement) updated = True log(f"Updated in table cell: {original} => {replacement}") for section in doc.sections: for part in (section.header, section.footer): if part: for para in part.paragraphs: for original, replacement, *_ in links_to_update: if original in para.text: para.text = para.text.replace(original, replacement) updated = True log(f"Updated in header/footer: {original} => {replacement}") if updated: doc.save(file_path) log(f"Saved updated file: {file_path}") return True log(f"No updates made to file: {file_path}") return False except Exception as e: log(f"Error updating DOCX file {file_path}: {e}") return False

def scan_and_update_documents(base_path): processed_files = 0 updated_files = 0 total_replacements = 0 error_files = [] log(f"\nScanning and updating: {base_path}\n") for root, , files in os.walk(base_path): for file in files: ext = os.path.splitext(file)[1].lower() if ext not in (".docx",): continue full_path = os.path.join(root, file) processed_files += 1 log(f"[{processed_files}] Processing: {full_path}") try: links = extract_docx_links(full_path) if not links: log("  No replaceable links found") continue replaceable_links = [(orig, repl) for orig, repl, * in links if repl] if not replaceable_links: log("  No valid replacements available") continue if update_docx_file(full_path, replaceable_links): updated_files += 1 total_replacements += len(replaceable_links) log(f"  Updated {len(replaceable_links)} links") except Exception as e: error_files.append((full_path, str(e))) log(f"[ERROR] {file}: {e}") log("\n=== Scan Complete ===") log(f"Processed files: {processed_files}") log(f"Updated files: {updated_files}") log(f"Total replacements: {total_replacements}") if error_files: log("\nErrors encountered:") for file, error in error_files: log(f"  {file}: {error}")

if name == "main": folder_path = input("Enter the folder path to scan and update Word documents: ").strip() if not folder_path or not os.path.exists(folder_path): print("Invalid path. Please provide a valid directory.") sys.exit(1) scan_and_update_documents(folder_path)

